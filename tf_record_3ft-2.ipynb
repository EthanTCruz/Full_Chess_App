{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed25669-2a2c-4518-98f7-2fdc6c0de653",
   "metadata": {},
   "source": [
    "# Revised TF Record Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf6fbc5-3fb3-4fad-b8e7-45d2af89ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\git\\Full_Chess_App\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import chess\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import  Session\n",
    "from tqdm import tqdm\n",
    "from Chess_Model.src.model.classes.cnn_scorer import boardCnnEval\n",
    "from joblib import load\n",
    "import math\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "# print output to the console\n",
    "print(current_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff98ff5-17fb-40d5-9d67-d2bb828a7ea6",
   "metadata": {},
   "source": [
    "# Importing my stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad337cd0-1b8e-4cd3-86e7-e83ed36fac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chess_Model.src.model.config.config import Settings\n",
    "from Chess_Model.src.model.classes.sqlite.dependencies import  fetch_one_game_position, fetch_all_game_positions_rollup,get_rollup_row_count,board_to_GamePostition\n",
    "from Chess_Model.src.model.classes.sqlite.models import GamePositions\n",
    "from Chess_Model.src.model.classes.sqlite.database import SessionLocal\n",
    "from Chess_Model.src.model.classes.cnn_scorer import boardCnnEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81413b56-614d-4c7b-b91f-a8cd49eff7c5",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c5bb08-36c5-49c9-a44c-a7d27b2c8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_file_name = 'combined_data.tfrecord'\n",
    "metadata_key = 'metadata'\n",
    "bitboards_key = 'positions_data'\n",
    "results_key = 'game_results'\n",
    "recordsDir = \"./Chess_Model/src/model/data/\"\n",
    "recordsData: str = f\"{recordsDir}feature_data.tfrecord\"\n",
    "recordsDataCopy: str = f\"{recordsDir}feature_data_copy.tfrecord\"\n",
    "recordsDataTrain = f\"{recordsDir}train_data.tfrecord\" \n",
    "recordsDataValid = f\"{recordsDir}validation_data.tfrecord\"\n",
    "recordsDataTest: str = f\"{recordsDir}test_data.tfrecord\"\n",
    "feature_description = {\n",
    "    'bitboards': tf.io.FixedLenFeature([], tf.string),\n",
    "    'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "    'target': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "scalarFile = './Chess_Model/src/model/data/scaler.joblib'\n",
    "evaluator = boardCnnEval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40805369-1e72-4915-b40b-da0ae0ec7a47",
   "metadata": {},
   "source": [
    "## Part 1: Pipeline from SqLite DB to Record File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba544a5-2028-4e5e-bc0d-2be2f54486a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_board_scores(scores_dict: dict):\n",
    "\n",
    "    metadata = list(scores_dict[metadata_key].values())\n",
    "\n",
    "    bitboards = list(scores_dict[bitboards_key].values())\n",
    "    \n",
    "\n",
    "    game_results = list(scores_dict[results_key].values())\n",
    "    \n",
    "    return bitboards, metadata, game_results\n",
    "    \n",
    "def serialize_data(scores_dict):\n",
    "\n",
    "        bb,md,gr = split_board_scores(scores_dict)\n",
    "        \n",
    "        bitboards_tensor = tf.stack([tf.convert_to_tensor(board, dtype=tf.int8) for board in bb])\n",
    "        serialized_bitboards = tf.io.serialize_tensor(bitboards_tensor)\n",
    "        \n",
    "        metadata_tensor = tf.convert_to_tensor(md,dtype=tf.float16)\n",
    "        serialized_metadata = serialize_tensor(metadata_tensor)\n",
    "        \n",
    "        target_tensor = tf.convert_to_tensor(gr,dtype=tf.float16)\n",
    "        serialized_target = serialize_tensor(target_tensor)\n",
    "        \n",
    "        return serialized_bitboards, serialized_metadata, serialized_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec7f6e5c-8b31-40aa-ac88-488af2ce1c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Feature Data: 100%|████████████████████████| 15045/15045 [02:15<00:00, 110.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_file(file):\n",
    "    # Check if the file exists and remove it\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    \n",
    "        # Create a new CSV file with the column headers\n",
    "    with open(file, 'w', newline='') as File:\n",
    "        pass\n",
    "        \n",
    "def process_sqlite_boards_to_records(batch_size: int = 5):\n",
    "    #initializes tf record file\n",
    "    create_file(recordsData)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(recordsData) as writer:\n",
    "        with SessionLocal() as db:\n",
    "            #get count for load bar\n",
    "            row_count = get_rollup_row_count(db=db)\n",
    "\n",
    "            #getter for generator for getting data from db\n",
    "            batch = fetch_all_game_positions_rollup(yield_size=500, db=db)\n",
    "            serialized_examples = []  # List to accumulate serialized examples\n",
    "            for game in tqdm(batch, total=row_count, desc=\"Processing Feature Data\"):\n",
    "                try:\n",
    "                    if game:\n",
    "                        #initialize evaluator to board\n",
    "                        evaluator.setup_parameters_gamepositions(game=game)\n",
    "                        #get scores dict\n",
    "                        score = evaluator.get_board_scores_records()\n",
    "\n",
    "                        #serialize data for saving\n",
    "                        serialized_data = serialize_data(score)\n",
    "                        \n",
    "                        features = {\n",
    "                        'bitboards': _bytes_feature(serialized_data[0].numpy()),\n",
    "                        'metadata': _bytes_feature(serialized_data[1].numpy()),\n",
    "                        'target': _bytes_feature(serialized_data[2].numpy())\n",
    "                        }\n",
    "                        \n",
    "                        serialized_data = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "                        serialized_examples.append(serialized_data)\n",
    "\n",
    "                        # Check if we've accumulated enough examples to write a batch\n",
    "                        if len(serialized_examples) >= batch_size:\n",
    "                            for serialized_example in serialized_examples:\n",
    "                                writer.write(serialized_example.SerializeToString())\n",
    "                            serialized_examples = []  # Reset the list after writing\n",
    "                    else:\n",
    "                        return 1\n",
    "                except Exception as e:\n",
    "                    raise Exception(e)\n",
    "            \n",
    "            # Write any remaining examples after looping through all games\n",
    "            for serialized_example in serialized_examples:\n",
    "                writer.write(serialized_example)\n",
    "process_sqlite_boards_to_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1beed29-a94e-4deb-b243-e553d594c9a4",
   "metadata": {},
   "source": [
    "## Part 2: Shuffle/Split file into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f951f-a23c-4444-aad7-ab829942de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_csv(source_file, destination_file):\n",
    "    shutil.copy(source_file, destination_file)\n",
    "\n",
    "def split_csv(chunksize=10000):\n",
    "        if os.path.exists(train_file):\n",
    "            os.remove(train_file)\n",
    "        if os.path.exists(test_file):\n",
    "            os.remove(test_file)\n",
    "        if os.path.exists(validation_file):\n",
    "            os.remove(validation_file)\n",
    "        if os.path.exists(copy_data):\n",
    "            os.remove(copy_data)\n",
    "        copy_csv(source_file=filename, destination_file=copy_data)\n",
    "        \n",
    "        filename = copy_data\n",
    "        total_rows = get_row_count(filename=filename)\n",
    "\n",
    "        #make sure no shared inices\n",
    "        # Split indices for training+testing and validation\n",
    "        validation_size = validation_size  # 20% of the data for validation\n",
    "        train_test_indices = set(range(total_rows))\n",
    "        validation_indices = set(random.sample(list(train_test_indices), int(total_rows * validation_size)))\n",
    "\n",
    "        train_test_indices -= validation_indices  # Remove validation indices from training+testing pool\n",
    "\n",
    "        # Further split training+testing indices into training and testing\n",
    "        test_indices = set(random.sample(list(train_test_indices), int(len(train_test_indices) * test_size)))\n",
    "\n",
    "        processed_rows = 0\n",
    "\n",
    "\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "\n",
    "\n",
    "            chunk_train = chunk.iloc[[i - processed_rows in train_test_indices and i - processed_rows not in test_indices for i in range(processed_rows, processed_rows + len(chunk))]]\n",
    "            chunk_test = chunk.iloc[[i - processed_rows in test_indices for i in range(processed_rows, processed_rows + len(chunk))]]\n",
    "            chunk_validation = chunk.iloc[[i - processed_rows in validation_indices for i in range(processed_rows, processed_rows + len(chunk))]]\n",
    "\n",
    "            # Write to respective files\n",
    "            mode = 'a' if processed_rows > 0 else 'w'\n",
    "            chunk_train.to_csv(train_file, mode=mode, index=False, header=(mode == 'w'))\n",
    "            chunk_test.to_csv(test_file, mode=mode, index=False, header=(mode == 'w'))\n",
    "            chunk_validation.to_csv(validation_file, mode=mode, index=False, header=(mode == 'w'))\n",
    "\n",
    "            # Update processed rows counter\n",
    "            processed_rows += len(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bb3b3-b27f-4b98-a38a-4ff944a74b99",
   "metadata": {},
   "source": [
    "## Part 3: Create Base Parse Function for records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164c25a-4ef7-46c6-a923-2c61ce06ba02",
   "metadata": {},
   "source": [
    "## Part 4: Create scaler from train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5927175-6d29-4a02-8a93-a19c5e29a026",
   "metadata": {},
   "source": [
    "## Part 5: Create scaled parse function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a283023-acf7-403a-8ee3-463d1acb59ed",
   "metadata": {},
   "source": [
    "## Part 6: Calculate shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b73b6f-0f07-45c4-b4b0-e3f4cab50a0d",
   "metadata": {},
   "source": [
    "## Part 7: Create tf dataset with scaled parse function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef3d5f0-21a2-4d2e-8b2b-38926ca634c6",
   "metadata": {},
   "source": [
    "## Part 8: Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abf51-ed5e-4653-9f4f-e45549ea7740",
   "metadata": {},
   "source": [
    "## Part 9: Pass tf dataset into model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077298ba-3879-422b-9495-f4183c4c9559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62ab9ad-c301-462b-93f4-fd0367b78895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_tensor(tensor):\n",
    "    return tf.io.serialize_tensor(tf.convert_to_tensor(tensor, dtype=tensor.dtype))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chessModel)",
   "language": "python",
   "name": "chessmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
